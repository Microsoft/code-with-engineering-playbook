# Synthetic Monitoring Tests

Synthetic Monitoring Tests are a set of functional tests that run alongside the main product to verify its health and resilience, at runtime.

## Why Synthetic Monitoring tests [The Why]

Traditionally, software providers rely on testing through CI/CD stages in the well known [testing pyramid](https://martinfowler.com/bliki/TestPyramid.html) (unit, integration, e2e) to validate that the product is healthy and without regressions. Such tests will run on the build agent or in the test/stage environment before being deployed to production and released to live user traffic. During the services' lifetime in the production environment, they are safeguarded by monitoring and alerting tools that rely on Real User Metrics/Monitoring ([RUM](https://en.wikipedia.org/wiki/Real_user_monitoring)).

However, as more organizations today provide highly-available (99.9+ SLA) products, they find that the nature of distributed applications (hardware or software) over time is to fail. Moreover, frequent releases to parts the system generate further instability to the system and making the production environment not hermetic and actually not representative to any given version in the source control.

For such systems, the ambition of service engineering teams is to reduce to a minimum the time it takes to fix errors, or the [MTTR - Mean Time To Repair](https://en.wikipedia.org/wiki/Mean_time_to_repair). It is a continuous effort, performed on the live/production system.

Synthetic Monitoring tests are a subset of tests that may run in production, sometimes named Test-in-Production or Shift-Right tests. Shift right compliments and add on top of the Shift-Left paradigms that are so popular, providing modern service-engineering teams a broader set of tools to assure high SLAs over time.

## Synthetic Monitoring tests Design Blocks [The What]

Testing using synthetic data injects user behaviors to the system and validates their effect, usually by passively relying on existing monitoring and alerting capabilities.
Components of synthetic monitoring tests include **Probes**, test code which generates data, and **Monitoring tools** placed to validate both the system's behavior under test and the health of the probes themselves.

[TODO: ADD IMAGE]

### Probes

A Synthetic Monitoring test is, in fact, very related to black-box tests and would usually focus on end-to-end scenarios from a user's perspective.
It is not uncommon for the same code for e2e or integration tests to be used to implement the probe.

### Monitoring

Given that Synthetic Monitoring tests are continuously running, at intervals, in a production environment, the assertion of system behavior through analysis relies on existing monitoring fundamentals used for the live system (Loggin, Metrics, Distributed Tracing).
There would usually be a finite set of tests, and key metrics that assert are used to build monitors and alerts against the known [SLO](https://en.wikipedia.org/wiki/Service-level_objective) and verify that the [OKR](https://en.wikipedia.org/wiki/OKR) for that system are maintained. The monitoring tools are effectively capturing both RUMs and synthetic data generated by the probes.

## Applying Tests in Production [the how]

### Asserting the system under tests

The nature of synthetic monitors, as with application performance monitoring, is statistical. Test metrics are usually compared against some historical or running average with a time dimension. So if a measurement is within a deviation of the norm at any time, the services are probably healthy.

### Monitoring the health of tests

Probes runtime is a production environment on its own, and the health of tests is critical. Many providers offer cloud-based systems that provide such runtime, while some organizations use existing production environments to run these tests on. That said, a monitor-the-monitor strategy must be a first-class citizen of the production environment's alerting systems.

### Risks

Testing in production, in general, has a risk factor attached to it, which does not exist tests executed during CI/CD stages. Specifically, in synthetic monitoring tests, the following may affect the production environment:

* Corrupted or invalid data - Tests inject test data which may be in some way corrupt. Consider using a testing schema.
* Protected data leakage - Tests run in a production environment and emit logs or tracing that may contain protected data.
* Overloaded systems - Synthetic tests may cause errors or overload the system themselves.
* Unintended side effects or impacts on other production systems.
* Skewed analytics (traffic funnels, A/B test results, etc.)
* Auth/AuthZ - Tests are required to run in production where access to auth/authZ tokens may be restricted or more challenging to retrieve.

## Synthetic Monitoring tests Frameworks and Tools

Most key monitoring/APM players have an enterprise product that supports synthetic monitoring built into their system (see list below), making many of the risks raised above irrelevant.
Some organizations prefer running probes on existing infrastructure using known tools such as [Postman](https://www.postman.com/), [Wrk](https://github.com/wg/wrk), [JMeter](https://jmeter.apache.org/), [Selenium](https://www.selenium.dev/) or even custom code to generate the synthetic data and rely on their existing tools to alert, monitor and maintain the tests themselves.

* [Application Insights availibility](https://docs.microsoft.com/en-us/azure/azure-monitor/app/monitor-web-app-availability) - Simple avaibility tests that allow some customization using [Multi-step web test](https://docs.microsoft.com/en-us/azure/azure-monitor/app/availability-multistep)
* [DataDog Synthetics](https://www.datadoghq.com/dg/apm/synthetics/api-test/)
* [Dynatrace Syntehtic Monitoring](https://www.dynatrace.com/platform/synthetic-monitoring/)
* [New Relic Synthetics](https://newrelic.com/products/synthetics)

## Conclusion

The value of production tests, in general, and specifically Synthetic monitoring, is only there for particular engagement types. However, when applicable, it is essential to consider them when designing the testing strategy.

...

In conclusion, provide the final thoughts on why and how this type of test can help with your next customer engagement, what best practices and recommendations that can be withdrawn from the case studies and research.

## Resources

[Google SRE book - Testing Reliability](https://landing.google.com/sre/sre-book/chapters/testing-reliability/)

[Microsoft DevOps Architectures - Shift Right to Test in Production](https://docs.microsoft.com/en-us/azure/devops/learn/devops-at-microsoft/shift-right-test-production)

[Martin Fowler - Synthetic Monitoring](https://martinfowler.com/bliki/SyntheticMonitoring.html)
