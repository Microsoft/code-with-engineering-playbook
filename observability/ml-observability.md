# Observability in Machine Learning  

List of proposed topics to cover:  

## Model experimentation and tuning : logging and dashboard  

here is about metrics logging, comparison various models, dashboards.

When developing and tuning machine learning models data scientist experiments with various parameters
and model performance metrics.
Tools that facilitate team of data scientists to observe and share modelling results are.

[MLFlow for Databricks](https://docs.microsoft.com/en-us/azure/databricks/applications/mlflow/)
Here is best practice to observe model behaiviour using MLFlow

[Azure Machine learning Service](https://ml.azure.com/)
Here is best practice to observe model behaiviour using AML  

## Model experimentation and tuning: reproducibility  

Here is about reproducible experimenation>  

## Model performance over time: data drift  

In re-tranining scenario model behaviour may degrade due to changes in data.
Here is how to observe and report model performance in production.

    Model deployment:
    Deployed as dependency (pickle file)  
    Deployed as service

## Data versioning  
